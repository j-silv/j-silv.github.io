---
title: "VAE molecule generation"
description: Replication of A Two-Step Graph Convolutional Decoder for Molecule Generation paper by Bresson et Laurent (2019)
categories: [Software]
tags: [ml, chemistry, ai]
pin: true
media_subpath: /assets/img/molegen
math: true
image:
  path: /flowchart.png
  alt: Model architecture
---

[![GitHub last commit](https://img.shields.io/github/last-commit/j-silv/molegen?style=for-the-badge&logo=github&color=black
)](https://github.com/j-silv/molegen)

[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face%20Space-black?style=for-the-badge&color=black)](https://huggingface.co/spaces/j-silv/molegen)

## Introduction

In this post, I discuss how I replicated the model and results from the 2019 ML paper: [A Two-Step Graph Convolutional Decoder for Molecule Generation](https://arxiv.org/pdf/1906.03412).

## Literature read

I started by reading the paper. As illustrated in the flowchart above, the authors use the following approach to train the ML model:

1. Given an input SMILES string, convert it to a numerical graph representation (vertices are atoms, edges are bonds)
2. Train a graph convolutional network on the graph (more specifically, a [Residual Gated Graph ConvNet](https://arxiv.org/abs/1711.07553)). This step then generates a latent representation of the molecule (graph) which is a single summary vector $z$
3. Using $z$, train a neural network classifier to determine the number of atoms for each predicted atom type (e.g. 3 carbon and 1 oxygen atom). The authors refer to this as a 'bag-of-atoms' matrix
4. Using the bag-of-atoms matrix, create a fully connected graph such that all atoms (vertices) are connected to each other 
5. Together with $z$, the bag-of-atoms matrix, and this fully connected graph, train another Graph ConvNet to generate a bond (edge) level feature prediction
6. Determine loss from ground truth bag-of-atoms and actual edge features (single bond, double bond, etc.)

One additional constraint is that latent representation should be Gaussian. Or more formally, $z$ represents the distribution of the latent space which should be as close to Gaussian as possible. This formulation is known as a [Variational Autoencoder](https://arxiv.org/abs/1906.02691) and it allows us to sample from a unit Gaussian distribution to generate molecules. For inference, the authors use the following approach:

1. Sample from a unit Gaussian distribution to generate the latent representation $z$
2. Generate the bag-of-atoms matrix and the bond features similar to step 3-5 above

There are additional considerations such as positional encoding and edge probability loss calculations which will be covered in the following sections. The paper also discusses additional post-generative steps including beam search and molecule validity checks, but I ignored those for simplicity. 

## Tools and libraries

To replicate the paper, the following components were used:

- **Python** as the program language
- **Pandas** for processing the molecular dataset 
- **RDKit** for molecule specific functions such as SMILES conversion
- **PyTorch** for creating the model, training loop, and inference pipeline
- **PyTorch Geometric** for the graph-specific ML algorithms and data batching
- **Streamlit** for visualization 

As a first step to building the system, I accessed and explored the training dataset.

## Dataset exploration

The authors used a subset of the [ZINC dataset](https://zinc.docking.org/). Since I was already using PyTorch Geometric, I thought I could simply load in the [ZINC dataset](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.ZINC.html) which is bundled with the library. However, this method doesn't allow us to extract any SMILES or molecule information directly since the atom and bond feature mappings are not explicitly shared. Instead, I did my own processing on raw SMILES string with a dataset I found on Kaggle: [ZINC-250k dataset](https://www.kaggle.com/datasets/basu369victor/zinc250k). For debugging, I loaded a small subset of 1000 molecules.

You can see how the data and relevant features were processed with the following widget:

<iframe
       id="molegen-data"
       src="https://j-silv-molegen.hf.space/data?embed=true"
       frameborder="0"
       width="100%"
       height=100vh
></iframe>
<script src="https://cdn.jsdelivr.net/npm/iframe-resizer@4.3.4/js/iframeResizer.min.js"></script>
<script>
  iFrameResize({license: 'GPLv3', waitForLoad: true,
}, "#molegen-data")
</script>

## _I'm actively working on the rest of the post! Check back soon..._

## References

A huge thanks to the following authors for writing introductory tutorials on Variational Autoencoders: 

- [Eric Jang](https://blog.evjang.com/2016/08/variational-bayes.html)
- [Alexander Van de Kleut](https://avandekleut.github.io/vae/)
- [Lilian Weng](https://lilianweng.github.io/posts/2018-08-12-vae/)
- [Reza Kalantar](https://medium.com/@rekalantar/variational-auto-encoder-vae-pytorch-tutorial-dce2d2fe0f5f)
- [Hunter Heidenreich](https://hunterheidenreich.com/posts/modern-variational-autoencoder-in-pytorch/)
- [Aqeel Anwar](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2/)